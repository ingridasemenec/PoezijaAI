{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read poems from the specified folder and split by lines\n",
    "folder_path = \"C:/Users/josep/OneDrive/Desktop/Erdos/poezija/poezija/\"\n",
    "corpus = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()\n",
    "            content = content.replace(\"\\n\", \" \\n \")  # Ensure newline is treated as a separate token\n",
    "            poems = content.split(\" \\n \")  # Split by newlines to treat each line as a separate poem\n",
    "            corpus.extend(poems)  # Add each poem line to the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word index dictionary from the corpus\n",
    "word_index = {}\n",
    "index_counter = 1  # Start at 1 to reserve 0 for the PAD token\n",
    "\n",
    "for poem in corpus:\n",
    "    for word in poem.split():\n",
    "        if word not in word_index:\n",
    "            word_index[word] = index_counter\n",
    "            index_counter += 1\n",
    "\n",
    "# Define PAD token\n",
    "PAD_TOKEN_ID = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data, word_index):\n",
    "        self.data = data\n",
    "        self.word_index = word_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = [self.word_index[word] for word in self.data[idx].split()]\n",
    "        target_seq = input_seq[1:] + [PAD_TOKEN_ID]  # Shifted target sequence\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and dataloader\n",
    "dataset = PoemDataset(corpus, word_index)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: collate_fn(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    return inputs_padded, targets_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(PoemGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_TOKEN_ID)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device),\n",
    "                weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            # Move inputs and targets to device\n",
    "            inputs, targets = inputs.long().to(device), targets.long().to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.transpose(1, 2), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(word_index) + 1  # Include PAD token\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = PoemGenerator(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)  # Ignore the PAD token in loss computation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.6251\n",
      "Epoch 2/20, Loss: 1.4360\n",
      "Epoch 3/20, Loss: 1.3035\n",
      "Epoch 4/20, Loss: 1.2010\n",
      "Epoch 5/20, Loss: 1.1281\n",
      "Epoch 6/20, Loss: 1.0757\n",
      "Epoch 7/20, Loss: 1.0384\n",
      "Epoch 8/20, Loss: 1.0161\n",
      "Epoch 9/20, Loss: 0.9969\n",
      "Epoch 10/20, Loss: 0.9802\n",
      "Epoch 11/20, Loss: 0.9673\n",
      "Epoch 12/20, Loss: 0.9583\n",
      "Epoch 13/20, Loss: 0.9511\n",
      "Epoch 14/20, Loss: 0.9466\n",
      "Epoch 15/20, Loss: 0.9362\n",
      "Epoch 16/20, Loss: 0.9306\n",
      "Epoch 17/20, Loss: 0.9259\n",
      "Epoch 18/20, Loss: 0.9180\n",
      "Epoch 19/20, Loss: 0.9142\n",
      "Epoch 20/20, Loss: 0.9126\n"
     ]
    }
   ],
   "source": [
    "train_model(model, data_loader, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(model, start_sequence, word_index, max_len=50, temperature=1.0, top_p=0.9):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([word_index.get(word, PAD_TOKEN_ID) for word in start_sequence.split()], dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    poem = start_sequence.split()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model(input_seq.unsqueeze(0), hidden)\n",
    "            output = output[:, -1, :].squeeze(0)  # Take the last output\n",
    "\n",
    "            # Apply temperature\n",
    "            logits = output / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sort the probabilities to apply top-p sampling\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above top_p\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "\n",
    "            # Handle the case where all probabilities are zero\n",
    "            if sorted_probs.sum().item() == 0:\n",
    "                sorted_probs = torch.ones_like(sorted_probs) / len(sorted_probs)\n",
    "\n",
    "            # Normalize remaining probabilities\n",
    "            sorted_probs /= sorted_probs.sum()\n",
    "\n",
    "            # Ensure no negative values or NaNs\n",
    "            if torch.any(sorted_probs < 0) or torch.isnan(sorted_probs).any() or torch.isinf(sorted_probs).any():\n",
    "                raise ValueError(\"Invalid values in probabilities after normalization\")\n",
    "\n",
    "            # Sample from the filtered distribution\n",
    "            word_id = sorted_indices[torch.multinomial(sorted_probs, 1).item()].item()\n",
    "\n",
    "            # Convert word_id to word and add to poem\n",
    "            for word, idx in word_index.items():\n",
    "                if idx == word_id:\n",
    "                    poem.append(word)\n",
    "                    break\n",
    "\n",
    "            # Update the input sequence\n",
    "            input_seq = torch.tensor([word_id], dtype=torch.long).to(device)\n",
    "\n",
    "    return ' '.join(poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mano saujoj gesta rytas - - turėjo numauta siela. akių. gyvastį lino, akių. tilto, bežadė užpuolimo styrint sau! plienas - aidas pat, žaliavo; o per fotelį, kraujo! - legendos. tolyje vakarus! fotelį, apsijuokęs. atbėgančiam Nevėžis - tenai per tolyje per tolyje piemenio, nesumanytų dulkės, čia vanduo. klausiam - Brazilijoj kelio kilimo, sudužusių, vėjelis, lengva“\n"
     ]
    }
   ],
   "source": [
    "generated_poem = generate_poem(model, start_sequence=\"mano saujoj gesta rytas\", word_index=word_index, temperature=0.6, top_p=0.95)\n",
    "print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mano saujoj gesta rytas - -\n",
      "miela! paparčiuos - bepaliks - Brazilijoj\n",
      "aukštį kitaip rugius, pasipūtę bėgo! Kuri\n",
      "linksmybe. suprastumėm: - matai plaukus o\n",
      "apie tyli d'antan! liepia: - akių.\n",
      "- šakas. dausos! malonus daina. ji...\n",
      "o bežadė paiko! Nuskendau - skleisti\n",
      "mirtį, prilygt - akių. žmonės, gražų\n",
      "- - pastogės, legendos. visuotinio, visgi\n"
     ]
    }
   ],
   "source": [
    "def format_poem(poem, max_words_per_line=6):\n",
    "    words = poem.split()  # Split the poem into individual words\n",
    "    lines = []\n",
    "    line = []\n",
    "    \n",
    "    for word in words:\n",
    "        line.append(word)\n",
    "        if len(line) >= max_words_per_line or word in [\".\", \"!\", \"?\"]:  # Break after max words or punctuation\n",
    "            lines.append(' '.join(line))\n",
    "            line = []\n",
    "    \n",
    "    # Add the remaining words as the last line\n",
    "    if line:\n",
    "        lines.append(' '.join(line))\n",
    "    \n",
    "    return '\\n'.join(lines)  # Join the lines with a newline to format it like a poem\n",
    "\n",
    "# Example usage\n",
    "generated_poem = generate_poem(model, start_sequence=\"mano saujoj gesta rytas\", word_index=word_index, temperature=0.6, top_p=0.95)\n",
    "formatted_poem = format_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daina aidi tolumoj angelas klysti Šiaulių\n",
      "idant - idant jo perone pavydi,\n",
      "šneku. po kojų Capri: - žiūriu\n",
      "o tarpu idant – per nakties!\n",
      "kraujo! putojantį nesumanytų - vanduo. tenai\n",
      "o stiklą putojantį klaidą skausmai. -\n",
      "putojantį oranžinėm - lietų o gieda\n",
      "- matai – – – Ir\n",
      "banga, per lauko plauks. šalį,\n"
     ]
    }
   ],
   "source": [
    "def format_poem(poem, max_words_per_line=6):\n",
    "    words = poem.split()  # Split the poem into individual words\n",
    "    lines = []\n",
    "    line = []\n",
    "    \n",
    "    for word in words:\n",
    "        line.append(word)\n",
    "        if len(line) >= max_words_per_line or word in [\".\", \"!\", \"?\"]:  # Break after max words or punctuation\n",
    "            lines.append(' '.join(line))\n",
    "            line = []\n",
    "    \n",
    "    # Add the remaining words as the last line\n",
    "    if line:\n",
    "        lines.append(' '.join(line))\n",
    "    \n",
    "    return '\\n'.join(lines)  # Join the lines with a newline to format it like a poem\n",
    "\n",
    "# Example usage\n",
    "generated_poem = generate_poem(model, start_sequence=\"daina aidi tolumoj\", word_index=word_index, temperature=0.6, top_p=0.95)\n",
    "formatted_poem = format_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_sp_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
