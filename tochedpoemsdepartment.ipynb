{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read poems \n",
    "folder_path = \"C:/Users/josep/OneDrive/Desktop/Erdos/poezija/poezija/\"\n",
    "corpus = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(root, filename), 'r', encoding=\"utf-8\") as file:\n",
    "                content = file.read().strip()\n",
    "                content = content.replace(\"\\n\", \" \\n \")  # Ensure newline is treated as a separate token\n",
    "                poems = content.split(\" \\n \")  # Split by newlines to treat each line as a separate poem\n",
    "                corpus.extend(poems)  # Add each poem line to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word index dictionary from the corpus\n",
    "word_index = {}\n",
    "index_counter = 1  # Start at 1 to reserve 0 for the PAD token\n",
    "\n",
    "for poem in corpus:\n",
    "    for word in poem.split():\n",
    "        if word not in word_index:\n",
    "            word_index[word] = index_counter\n",
    "            index_counter += 1\n",
    "\n",
    "# Define PAD token\n",
    "PAD_TOKEN_ID = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data, word_index):\n",
    "        self.data = data\n",
    "        self.word_index = word_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = [self.word_index[word] for word in self.data[idx].split()]\n",
    "        target_seq = input_seq[1:] + [PAD_TOKEN_ID]  # Shifted target sequence\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and dataloader\n",
    "dataset = PoemDataset(corpus, word_index)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: collate_fn(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    return inputs_padded, targets_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(PoemGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_TOKEN_ID)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device),\n",
    "                weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # What to monitor (this could be training or validation loss)\n",
    "    patience=5,  # Number of epochs to wait before stopping\n",
    "    mode='min',  # 'min' for loss (we want to minimize loss)\n",
    "    min_delta=0.001  # Minimum change to qualify as an improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            # Move inputs and targets to device\n",
    "            inputs, targets = inputs.long().to(device), targets.long().to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.transpose(1, 2), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(word_index) + 1  # Include PAD token\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = PoemGenerator(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)  # Ignore the PAD token in loss computation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.0193\n",
      "Epoch 2/30, Loss: 1.0164\n",
      "Epoch 3/30, Loss: 1.0135\n",
      "Epoch 4/30, Loss: 1.0107\n",
      "Epoch 5/30, Loss: 1.0057\n",
      "Epoch 6/30, Loss: 1.0027\n",
      "Epoch 7/30, Loss: 0.9992\n",
      "Epoch 8/30, Loss: 0.9954\n",
      "Epoch 9/30, Loss: 0.9929\n",
      "Epoch 10/30, Loss: 0.9929\n",
      "Epoch 11/30, Loss: 0.9873\n",
      "Epoch 12/30, Loss: 0.9830\n",
      "Epoch 13/30, Loss: 0.9810\n",
      "Epoch 14/30, Loss: 0.9793\n",
      "Epoch 15/30, Loss: 0.9772\n",
      "Epoch 16/30, Loss: 0.9757\n",
      "Epoch 17/30, Loss: 0.9716\n",
      "Epoch 18/30, Loss: 0.9703\n",
      "Epoch 19/30, Loss: 0.9684\n",
      "Epoch 20/30, Loss: 0.9717\n",
      "Epoch 21/30, Loss: 0.9675\n",
      "Epoch 22/30, Loss: 0.9606\n",
      "Epoch 23/30, Loss: 0.9593\n",
      "Epoch 24/30, Loss: 0.9557\n",
      "Epoch 25/30, Loss: 0.9552\n",
      "Epoch 26/30, Loss: 0.9535\n",
      "Epoch 27/30, Loss: 0.9527\n",
      "Epoch 28/30, Loss: 0.9503\n",
      "Epoch 29/30, Loss: 0.9494\n",
      "Epoch 30/30, Loss: 0.9550\n"
     ]
    }
   ],
   "source": [
    "train_model(model, data_loader, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(model, start_sequence, word_index, max_len=50, temperature=1.0, top_p=0.9):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([word_index.get(word, PAD_TOKEN_ID) for word in start_sequence.split()], dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    poem = start_sequence.split()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model(input_seq.unsqueeze(0), hidden)\n",
    "            output = output[:, -1, :].squeeze(0)  # Take the last output\n",
    "\n",
    "            # Apply temperature\n",
    "            logits = output / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sort the probabilities to apply top-p sampling\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above top_p\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "\n",
    "            # Handle the case where all probabilities are zero\n",
    "            if sorted_probs.sum().item() == 0:\n",
    "                sorted_probs = torch.ones_like(sorted_probs) / len(sorted_probs)\n",
    "\n",
    "            # Normalize remaining probabilities\n",
    "            sorted_probs /= sorted_probs.sum()\n",
    "\n",
    "            # Ensure no negative values or NaNs\n",
    "            if torch.any(sorted_probs < 0) or torch.isnan(sorted_probs).any() or torch.isinf(sorted_probs).any():\n",
    "                raise ValueError(\"Invalid values in probabilities after normalization\")\n",
    "\n",
    "            # Sample from the filtered distribution\n",
    "            word_id = sorted_indices[torch.multinomial(sorted_probs, 1).item()].item()\n",
    "\n",
    "            # Convert word_id to word and add to poem\n",
    "            for word, idx in word_index.items():\n",
    "                if idx == word_id:\n",
    "                    poem.append(word)\n",
    "                    break\n",
    "\n",
    "            # Update the input sequence\n",
    "            input_seq = torch.tensor([word_id], dtype=torch.long).to(device)\n",
    "\n",
    "    return ' '.join(poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mano saujoj gesta rytas - - turėjo numauta siela. akių. gyvastį lino, akių. tilto, bežadė užpuolimo styrint sau! plienas - aidas pat, žaliavo; o per fotelį, kraujo! - legendos. tolyje vakarus! fotelį, apsijuokęs. atbėgančiam Nevėžis - tenai per tolyje per tolyje piemenio, nesumanytų dulkės, čia vanduo. klausiam - Brazilijoj kelio kilimo, sudužusių, vėjelis, lengva“\n"
     ]
    }
   ],
   "source": [
    "generated_poem = generate_poem(model, start_sequence=\"mano saujoj gesta rytas\", word_index=word_index, temperature=0.6, top_p=0.95)\n",
    "print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mano saujoj gesta rytas durys, –\n",
      "akmuo į kalnus ir aguonėlės riksmą\n",
      "Visagali! savo pasaką visiškai pasaką virsta\n",
      "rašė aras apie kailius vandeny, riksmais,\n",
      "radio, savyje, tobulą apėmęs! o keturi\n",
      "giliai pasibaigė savo meilės girią, Karaliai\n",
      "ją apstu. gulbės siena. puolė ją\n",
      "tėra skrendanti Greičiau. jisai lūpų bet\n",
      "kaip keturi kareiviai, griūva, niekad aukso\n"
     ]
    }
   ],
   "source": [
    "def format_poem(poem, max_words_per_line=6):\n",
    "    words = poem.split()  # Split the poem into individual words\n",
    "    lines = []\n",
    "    line = []\n",
    "    \n",
    "    for word in words:\n",
    "        line.append(word)\n",
    "        if len(line) >= max_words_per_line or word in [\".\", \"!\", \"?\"]:  # Break after max words or punctuation\n",
    "            lines.append(' '.join(line))\n",
    "            line = []\n",
    "    \n",
    "    # Add the remaining words as the last line\n",
    "    if line:\n",
    "        lines.append(' '.join(line))\n",
    "    \n",
    "    return '\\n'.join(lines)  # Join the lines with a newline to format it like a poem\n",
    "\n",
    "generated_poem = generate_poem(model, start_sequence=\"mano saujoj gesta rytas\", word_index=word_index, temperature=0.6, top_p=0.95)\n",
    "formatted_poem = format_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daina aidi tolumoje laiką GRĮŽTU prieš\n",
      "mus šilainę ant žemės: šią būčiau\n",
      "prikelsi pušys matau: į krantą turtai\n",
      "jūros žiema!.. kur jų galvos. surašytų\n",
      "jos galvos. ir grimasa apačioj, nebus\n",
      "ir šviesos iš auksinių aukštyn, nukaltų\n",
      "stengiasi kelių adatą ima tįsti prašyt,\n",
      "į krantą laukiančias žydi kurių o\n",
      "nebeklauso liejas o keturi iškrenta\n"
     ]
    }
   ],
   "source": [
    "def format_poem(poem, max_words_per_line=6):\n",
    "    words = poem.split()  # Split the poem into individual words\n",
    "    lines = []\n",
    "    line = []\n",
    "    \n",
    "    for word in words:\n",
    "        line.append(word)\n",
    "        if len(line) >= max_words_per_line or word in [\".\", \"!\", \"?\"]:  # Break after max words or punctuation\n",
    "            lines.append(' '.join(line))\n",
    "            line = []\n",
    "    \n",
    "    # Add the remaining words as the last line\n",
    "    if line:\n",
    "        lines.append(' '.join(line))\n",
    "    \n",
    "    return '\\n'.join(lines)  # Join the lines with a newline to format it like a poem\n",
    "\n",
    "generated_poem = generate_poem(model, start_sequence=\"daina aidi tolumoje\", word_index=word_index, temperature=0.9, top_p=0.95)\n",
    "formatted_poem = format_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_sp_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
